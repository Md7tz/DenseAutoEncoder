{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3b59c1-8364-4608-8d91-298e78edfa4c",
   "metadata": {},
   "source": [
    "# A Simple Autoencoder for Anomaly Detection\n",
    "\n",
    "Anomaly detection is the task of finding anomalous data elements in a dataset. An anomaly is a data element that is an outlier with respect to the rest of the dataset.\n",
    "\n",
    "We are going to train an autoencoder on the MNIST dataset (that only contains numbers), and then we will look into anomalies within the MNIST dataset (i.e., images within MNIST that are somehow different than the rest of the dataset).\n",
    "\n",
    "Even though MNIST is a labeled dataset, we are going to disregard the labels for educational purposes and consider it as an unlabeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d8047a3-2391-44be-b2dc-a5d02b8376c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt | grep -v \"already\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a96f045a-acb0-454f-90ef-5920863c11ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True) # auto restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6678d9a8-4ffb-4dcf-bfc8-40dd5e81f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "# helpers\n",
    "from helpers import (get_data_loaders, seed_all)\n",
    "\n",
    "# Ensure reproducability\n",
    "seed_all(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5137d9e-d47a-4173-a0e1-aa16dd843d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 48000 examples for training and 12000 for validation\n",
      "Using 10 examples for testing\n"
     ]
    }
   ],
   "source": [
    "# This will get data loaders for the MNIST dataset for the train, validation\n",
    "# and test dataset\n",
    "data_loaders = get_data_loaders(batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f830f7-8af6-4799-8fb3-baee45d365bc",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e66f790a-c4a3-4ef9-bcda-6589f35af90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# obtain one batch\n",
    "dataiter = iter(data_loaders['train'])\n",
    "images, labels = next(dataiter)\n",
    "images = images.numpy()\n",
    "\n",
    "# Get one image from the batch\n",
    "# images[0], images[0].shape, images[0].ndim\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig, sub = plt.subplots(figsize=(2, 2))\n",
    "sub.imshow(img, cmap='gray')\n",
    "_ = sub.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f0c90-4622-419b-bee8-cba73d176e30",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear Autoencoder\n",
    "\n",
    "We'll train an autoencoder with these images by flattening them into vectors of length 784. The images from this dataset are already normalized such that the values are between 0 and 1.Sd a simple autoen archcoder. \n",
    "\n",
    "The encoder and decoder should be made of simple Multi-Layer Perceptrons. The units that connect the encoder and decoder will be the _compressed representation_ (also called _embedding_).\n",
    "\n",
    "Since the images are normalized between we d 1, you will need to use a **sigmoid activation on the output layer** to get values that match this input valueWexercise you are going to use a dimension for the embeddings of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66098d4d-3884-4dc0-835a-7e9d2738b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define NN Architecture\n",
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        ## encoder ##\n",
    "        # MLP with input size 28x28, output size encoding_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(in_features=256, out_features=encoding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(encoding_dim),\n",
    "        )\n",
    "\n",
    "        ## decoder ##\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=encoding_dim, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(in_features=256, out_features=28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.autoencoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            self.encoder,\n",
    "            self.decoder\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define feed-forward behaviour\n",
    "        # and scale the *output* layer with a sigmoid activation function\n",
    "        encoded = self.autoencoder(x)\n",
    "\n",
    "        # Reshape the output as an image\n",
    "        # the shape should be (batch_size, channel_count, height, width)\n",
    "        return encoded.reshape(encoded.shape[0], 1, 28, 28)\n",
    "\n",
    "\n",
    "# Initialize the NN\n",
    "encoding_dim = 32\n",
    "model = AutoEncoder(encoding_dim)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b98e0784-d2fc-4c7e-8318-b2d8571dd8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=256, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=256, out_features=784, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       "  (autoencoder): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Linear(in_features=256, out_features=32, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Linear(in_features=256, out_features=784, bias=True)\n",
       "      (4): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc98975-4c22-4503-8c41-7ce389a3c132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 256]         200,960\n",
      "            Linear-3                  [-1, 256]         200,960\n",
      "              ReLU-4                  [-1, 256]               0\n",
      "              ReLU-5                  [-1, 256]               0\n",
      "       BatchNorm1d-6                  [-1, 256]             512\n",
      "       BatchNorm1d-7                  [-1, 256]             512\n",
      "            Linear-8                   [-1, 32]           8,224\n",
      "            Linear-9                   [-1, 32]           8,224\n",
      "             ReLU-10                   [-1, 32]               0\n",
      "             ReLU-11                   [-1, 32]               0\n",
      "      BatchNorm1d-12                   [-1, 32]              64\n",
      "      BatchNorm1d-13                   [-1, 32]              64\n",
      "           Linear-14                  [-1, 256]           8,448\n",
      "           Linear-15                  [-1, 256]           8,448\n",
      "             ReLU-16                  [-1, 256]               0\n",
      "             ReLU-17                  [-1, 256]               0\n",
      "      BatchNorm1d-18                  [-1, 256]             512\n",
      "      BatchNorm1d-19                  [-1, 256]             512\n",
      "           Linear-20                  [-1, 784]         201,488\n",
      "           Linear-21                  [-1, 784]         201,488\n",
      "          Sigmoid-22                  [-1, 784]               0\n",
      "          Sigmoid-23                  [-1, 784]               0\n",
      "================================================================\n",
      "Total params: 840,416\n",
      "Trainable params: 840,416\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 3.21\n",
      "Estimated Total Size (MB): 3.26\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77353dcd-1013-4f19-ab94-7aebdba4f1c3",
   "metadata": {},
   "source": [
    "---\n",
    "## Loss Function\n",
    "We will use se the Mean Squared Error loss, which is called `MSELoss` in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c66e5cbb-cea9-4574-a315-b5f7b35d94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c827a2-94c7-4f2c-9572-16deddb47df7",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe8d8b93-85ac-4e92-b4a0-3c2efa9ef6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced47b60-bde3-4ed5-940d-f67acd817fae",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training loop is similar to a normal training loop, however, this task is an unsupervised task. That means we do not need labels. The MNIST dataset does provide labels, of course, so we will just disregard them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e81d45bb-79f9-41be-b0be-ebf5aea08ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:47<00:00,  1.02s/it]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 58.138537\tValid Loss: 31.390722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:13<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 27.254392\tValid Loss: 22.846437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:50<00:00,  1.07s/it]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:10<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 21.134707\tValid Loss: 18.517307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:47<00:00,  1.01s/it]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 17.240428\tValid Loss: 15.365045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 14.527420\tValid Loss: 13.362886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.00it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tTraining Loss: 12.843224\tValid Loss: 12.088553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tTraining Loss: 11.775808\tValid Loss: 11.296294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \tTraining Loss: 11.056461\tValid Loss: 10.716177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \tTraining Loss: 10.559689\tValid Loss: 10.203606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tTraining Loss: 10.039101\tValid Loss: 9.691781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.00it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \tTraining Loss: 9.657993\tValid Loss: 9.493472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \tTraining Loss: 9.249842\tValid Loss: 9.015205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.02it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:10<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \tTraining Loss: 8.901024\tValid Loss: 8.986780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.02it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \tTraining Loss: 8.722645\tValid Loss: 8.478968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:45<00:00,  1.02it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \tTraining Loss: 8.459555\tValid Loss: 8.315363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:45<00:00,  1.03it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 \tTraining Loss: 8.280913\tValid Loss: 8.447551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.02it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 \tTraining Loss: 8.120321\tValid Loss: 8.129892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 \tTraining Loss: 8.060010\tValid Loss: 8.319733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 \tTraining Loss: 8.009518\tValid Loss: 8.034884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:45<00:00,  1.02it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 \tTraining Loss: 7.880298\tValid Loss: 7.933388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 \tTraining Loss: 7.854605\tValid Loss: 7.916101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.02it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 \tTraining Loss: 7.751107\tValid Loss: 7.945728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \tTraining Loss: 7.797861\tValid Loss: 7.951730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.02it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 \tTraining Loss: 7.609475\tValid Loss: 7.616573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 \tTraining Loss: 7.587267\tValid Loss: 7.480736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.02it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 \tTraining Loss: 7.491342\tValid Loss: 7.413296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:45<00:00,  1.03it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 \tTraining Loss: 7.321323\tValid Loss: 7.363833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:45<00:00,  1.03it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 \tTraining Loss: 7.226991\tValid Loss: 7.630296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.00it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 \tTraining Loss: 7.219831\tValid Loss: 7.148285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 \tTraining Loss: 7.111612\tValid Loss: 7.146843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.00it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 \tTraining Loss: 6.991662\tValid Loss: 7.002075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 \tTraining Loss: 7.032791\tValid Loss: 7.175152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.02it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 \tTraining Loss: 7.008408\tValid Loss: 7.342382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.00it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 \tTraining Loss: 7.049790\tValid Loss: 7.126736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 \tTraining Loss: 6.965312\tValid Loss: 7.218203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.00it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 \tTraining Loss: 6.838730\tValid Loss: 6.926942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:47<00:00,  1.02s/it]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 \tTraining Loss: 6.893787\tValid Loss: 6.981691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.00it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 \tTraining Loss: 6.891105\tValid Loss: 7.039480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.02it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 \tTraining Loss: 6.807544\tValid Loss: 6.974291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:45<00:00,  1.02it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 \tTraining Loss: 6.804589\tValid Loss: 6.947637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.02it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 \tTraining Loss: 6.731888\tValid Loss: 6.928944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:47<00:00,  1.00s/it]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 \tTraining Loss: 6.674274\tValid Loss: 6.742896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 \tTraining Loss: 6.861020\tValid Loss: 7.191996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.02it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 \tTraining Loss: 6.712422\tValid Loss: 6.741435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:47<00:00,  1.00s/it]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 \tTraining Loss: 6.611104\tValid Loss: 6.786834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.00it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 \tTraining Loss: 6.645408\tValid Loss: 6.915473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 \tTraining Loss: 6.591982\tValid Loss: 6.716091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.00it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 \tTraining Loss: 6.642480\tValid Loss: 6.925700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.00it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:09<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 \tTraining Loss: 6.569583\tValid Loss: 6.638215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 47/47 [00:46<00:00,  1.01it/s]\n",
      "Validating: 100%|███████████████████████████████| 12/12 [00:08<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 \tTraining Loss: 6.537961\tValid Loss: 6.788415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # set to training mode\n",
    "    model.train()\n",
    "        \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    \n",
    "    # monitor training loss\n",
    "    train_loss = 0.\n",
    "    \n",
    "    for data in tqdm(\n",
    "        desc=\"Training\",\n",
    "        total=len(data_loaders['train']),\n",
    "        iterable=data_loaders['train'],\n",
    "        ncols=80,\n",
    "    ):\n",
    "        # disregard labels -> _\n",
    "        images, _ = data\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "\n",
    "        # Clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate the loss (between output and input instead of \"labels/targets\")\n",
    "        loss = criterion(outputs.flatten(), images.flatten())\n",
    "\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running training loss\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    ###################\n",
    "    #   Validation    #\n",
    "    ###################\n",
    "    # monitor validation loss\n",
    "    val_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(\n",
    "            desc=\"Validating\",\n",
    "            total=len(data_loaders['val']),\n",
    "            iterable=data_loaders['val'],\n",
    "            ncols=80,  \n",
    "        ):\n",
    "            # disregard labels -> _\n",
    "            images, _ = data\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "\n",
    "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate the loss (between output and input instead of \"labels/targets\")\n",
    "            loss = criterion(outputs.flatten(), images.flatten())\n",
    "\n",
    "            # Update running validation loss\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "    # Print avg training statistics \n",
    "    train_loss /= len(data_loaders['train'])\n",
    "    val_loss /= len(data_loaders['val'])\n",
    "\n",
    "    print(\"Epoch: {} \\tTraining Loss: {:.6f}\\tValid Loss: {:.6f}\".format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5aee85-36bb-4202-9a50-45f5a1e7a305",
   "metadata": {},
   "source": [
    "## Finding Anomalies\n",
    "Now that our autoencoder is trained we can use it to find anomalies. Let's consider the test set. We loop over all the batches in the test set and we record the value of the loss for each example separately. The examples with the highest reconstruction loss are our anomalies. \n",
    "\n",
    "Indeed, if the reconstruction loss is high, that means that our trained autoencoder could not reconstruct them well. Indeed, what the autoencoder learned about our dataset during training is not enough to describe these examples, which means they are different than what the encoder has seen during training, i.e., they are anomalies (or at least they are the most uncharacteristic examples).\n",
    "\n",
    "Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c61095ee-51c8-44ad-921f-dbb45a4f4eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>image</th>\n",
       "      <th>reconstructed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005403</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[[0.00063480873, 0.0013636247, 0.0005085095, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009877</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[[0.00031515013, 0.0016229856, 0.0009725884, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003008</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[[0.0013418869, 0.0016492341, 0.001286064, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007917</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[[0.0014731811, 0.002438784, 0.0020305591, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008114</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[[0.0022291497, 0.002851981, 0.0020182088, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss                                              image  \\\n",
       "0  0.005403  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "1  0.009877  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "2  0.003008  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "3  0.007917  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "4  0.008114  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                                       reconstructed  \n",
       "0  [[[0.00063480873, 0.0013636247, 0.0005085095, ...  \n",
       "1  [[[0.00031515013, 0.0016229856, 0.0009725884, ...  \n",
       "2  [[[0.0013418869, 0.0016492341, 0.001286064, 0....  \n",
       "3  [[[0.0014731811, 0.002438784, 0.0020305591, 0....  \n",
       "4  [[[0.0022291497, 0.002851981, 0.0020182088, 0....  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since this dataset is small we collect all the losses as well as\n",
    "# the image and its reconstruction in a dictionary. In case of a\n",
    "# larger dataset you might have to save on disk\n",
    "# (won't fit in memory)\n",
    "losses = {}\n",
    "\n",
    "# We need the loss by example (not by batch)\n",
    "loss_no_reduction = nn.MSELoss(reduction='none')\n",
    "\n",
    "idx = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(\n",
    "        desc=\"Testing\",\n",
    "        total=len(data_loaders['test']),\n",
    "        iterable=data_loaders['test'],\n",
    "        ncols=80,  \n",
    "    ):\n",
    "        # disregard labels -> _\n",
    "        images, _ = data\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "\n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate the loss (between output and input instead of \"labels/targets\")\n",
    "        loss = loss_no_reduction(outputs, images)\n",
    "        \n",
    "        # Accumulate results per-example\n",
    "        for i, l in enumerate(loss.mean(dim=[1, 2, 3])):\n",
    "            losses[idx + i] = {\n",
    "                'loss': float(l.cpu().numpy()),\n",
    "                'image': images[i].cpu().numpy(),\n",
    "                'reconstructed': outputs[i].cpu().numpy()\n",
    "            }\n",
    "\n",
    "        idx += loss.shape[0]\n",
    "        \n",
    "# Let's save our results in a pandas DataFrame\n",
    "df = pd.DataFrame(losses).T\n",
    "df.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120ddf4-328a-4b1d-8a63-a9a6fc891498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
